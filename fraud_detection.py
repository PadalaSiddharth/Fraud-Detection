# -*- coding: utf-8 -*-
"""Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kXvMPQ9BMqXy6m9zBfqjzk7GcoQFfd-K

Fraud detection

---

Installing and importing libraries
"""

!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install seaborn

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot
# %matplotlib inline

"""Loading the data set"""

df=pd.read_csv("Fraud.csv")
df

"""Handling null values and oultiers

"""

df = df.dropna()

from scipy import stats
z_scores = np.abs(stats.zscore(df[['amount', 'oldbalanceOrg', 'newbalanceOrig']]))
df_cleaned = df[(z_scores < 3).all(axis=1)]

import numpy as np
from scipy import stats

# Calculate z-scores and assign using .loc to avoid SettingWithCopyWarning
df.loc[:, 'z_score'] = np.abs(stats.zscore(df['amount'].fillna(0)))

# Define threshold
threshold = 3

# Identify outliers
outliers = df[df['z_score'] > threshold]
print(outliers)

# Filter the DataFrame to keep only rows with z_score <= threshold
df = df.loc[df['z_score'] <= threshold]

# Print the filtered DataFrame
df

from scipy import stats

# alculate z-scores and assign them using .loc
df.loc[:, 'z_score'] = stats.zscore(df['amount'].fillna(0))

# Define a threshold for outliers
thresholdC = 3

# Identify outliers based on z-scores
outliers_z_score = df[df['z_score'].abs() > threshold]

# Print outliers
print("Transactions identified as outliers based on Z-score:")
print(outliers_z_score[['amount', 'z_score']])

import numpy as np
from scipy import stats

# Calculate z-scores and assign using .loc to avoid SettingWithCopyWarning
df.loc[:, 'z_score'] = np.abs(stats.zscore(df['amount'].fillna(0)))

# Define threshold
threshold = 3

# Identify outliers
outliers = df[df['z_score'] > threshold]
print(outliers)

# Filter the DataFrame to keep only rows with z_score <= threshold
df = df.loc[df['z_score'] <= threshold]

# Print the filtered DataFrame
df

import numpy as np
from scipy import stats

# Calculate z-scores and assign using .loc to avoid SettingWithCopyWarning
df.loc[:, 'z_score'] = np.abs(stats.zscore(df['amount'].fillna(0)))

# Define threshold
threshold = 3

# Identify outliers
outliers = df[df['z_score'] > threshold]
print(outliers)

# Filter the DataFrame to keep only rows with z_score <= threshold
df = df.loc[df['z_score'] <= threshold]

# Print the filtered DataFrame
df

import matplotlib.pyplot as plt
# Select only numeric columns for correlation analysis
numeric_df = df.select_dtypes(include=[np.number])

# Compute the correlation matrix
corr_matrix = numeric_df.corr()

# Plot the heatmap to visualize correlations
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

# Identify pairs of features with correlation greater than 0.9
threshold = 0.9
high_corr_features = np.where(np.abs(corr_matrix) > threshold)
high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y]) for x, y in zip(*high_corr_features) if x != y and x < y]

# Print highly correlated pairs
print("Highly correlated feature pairs (correlation > 0.9):")
for pair in high_corr_pairs:
    print(pair)

df.loc[:, 'balanceOrg_diff'] = df['oldbalanceOrg'] - df['newbalanceOrig']
df.loc[:, 'balanceDest_diff'] = df['oldbalanceDest'] - df['newbalanceDest']

df.loc[:, 'isMerchant'] = df['nameDest'].apply(lambda x: 1 if x.startswith('M') else 0)

# Strip any leading/trailing spaces from column names
df.columns = df.columns.str.strip()

# Check column names again
print(df.columns)


df_encoded = pd.get_dummies(df, columns=['type'], drop_first=True)

df_encoded



"""Splitting null values"""

from sklearn.model_selection import train_test_split

X = df_encoded[['amount', 'balanceOrg_diff', 'balanceDest_diff', 'isMerchant', 'type_TRANSFER', 'type_CASH_OUT']]
y = df_encoded['isFraud']

"""Model selection Logistic Regression"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

log_reg = LogisticRegression(max_iter=1000, random_state=42)

# Fit the model
log_reg.fit(X_train, y_train)

# Predict on the test set
y_pred_log_reg = log_reg.predict(X_test)

# Evaluate the model
print("Logistic Regression")
print("Accuracy:", accuracy_score(y_test, y_pred_log_reg))
print("Classification Report:\n", classification_report(y_test, y_pred_log_reg))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# Initialize and fit the Logistic Regression model
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = log_reg.predict(X_test)

# Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Standardize features for better visualization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Get predicted probabilities
y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]

# Plot predicted probabilities versus actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_prob, y_test, alpha=0.5, c=y_test, cmap='bwr', edgecolor='k')
plt.xlabel('Predicted Probability of Fraud')
plt.ylabel('Actual Fraud (1) or Not (0)')
plt.title('Predicted Probabilities vs Actual Values')
plt.colorbar(label='Actual Fraud')
plt.show()

"""XGB Classifier"""

import xgboost as xgb
from xgboost import XGBClassifier

# Initialize the XGBoost Classifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Fit the model
xgb_model.fit(X_train, y_train)

# Predict on the test set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
print("XGBoost Classifier")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))

df_encoded

"""Random Forest Classifier"""

# Import necessary libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

# Split the data (assuming this part is already done)
# X = df[['amount', 'balanceOrg_diff', 'balanceDest_diff', 'isMerchant', 'type_TRANSFER', 'type_CASH_OUT']]
# y = df['isFraud']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)
y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Probabilities for ROC/AUC

# 1. Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# 2. Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 3. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Fraud", "Fraud"], yticklabels=["Not Fraud", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# 4. ROC Curve and AUC Score
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"Random Forest (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()

print(f"AUC Score: {roc_auc:.4f}")



df=pd.read_csv("Fraud.csv")
df

"""ROC Curve and Confusion matrix"""



# Import necessary libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


rf_classifier = RandomForestClassifier(n_estimators=100,max_depth=5, random_state=42)


rf_classifier.fit(X_train, y_train)


y_pred = rf_classifier.predict(X_test)
y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Get the probabilities for ROC AUC
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)
print(f"Accuracy: {accuracy:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
plt.title('Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

# Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()



"""Finding the Important Features"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming you've already trained the RandomForestClassifier
# rf_classifier = RandomForestClassifier(random_state=42)
# rf_classifier.fit(X_train, y_train)

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame for better readability
features = X.columns  # feature names
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

# Sort by importance (descending)
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Print the feature importance DataFrame
print(importance_df)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances - Random Forest Classifier')
plt.gca().invert_yaxis()  # Invert the y-axis to have the most important feature at the top
plt.show()



# Ensure predictions are made on the same subset
y_pred_anomalies = rf_classifier.predict(X_test)

# Add fraud alert to the correct DataFrame
df_test = df.loc[X_test.index]
df_test['fraud_alert'] = np.where(y_pred_anomalies == -1, 'Alert: Fraud Suspected', 'No Alert')

# Combine if needed
df_combined = pd.concat([df, df_test[['fraud_alert']]], axis=1)

"""Anomalies Detection"""



from sklearn.ensemble import IsolationForest
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Example dataset with transaction features
# df is your dataframe that contains transaction data
# Features: 'amount', 'balanceOrg_diff', 'balanceDest_diff', 'isMerchant', 'type_TRANSFER', 'type_CASH_OUT'

# Train an Isolation Forest model for anomaly detection (fraud detection)
iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)
iso_forest.fit(X_train)

# Predict anomalies on the test set
y_pred_anomalies = iso_forest.predict(X_test)

# In IsolationForest, -1 indicates anomaly (potential fraud), and 1 indicates normal behavior
# We need to ensure `y_pred_anomalies` is aligned with the DataFrame `df`
# If `df` is the full DataFrame, you should use the same indices for test data
# Create a DataFrame for test data to align with indices
df_test = X_test.copy()
df_test['fraud_alert'] = np.where(y_pred_anomalies == -1, 'Alert: Fraud Suspected', 'No Alert')

# Combine with original DataFrame if needed
df_combined = pd.concat([df, df_test[['fraud_alert']]], axis=1, join='inner')

# View potentially fraudulent transactions
fraud_transactions = df_combined[df_combined['fraud_alert'] == 'Alert: Fraud Suspected']

print("Potential Fraudulent Transactions:")
print(fraud_transactions)

# Visualize fraudulent vs non-fraudulent transactions
plt.figure(figsize=(10, 6))
plt.scatter(df_combined['amount'], df_combined['balanceOrg_diff'], c=(df_combined['fraud_alert'] == 'Alert: Fraud Suspected'), cmap='coolwarm', label='Fraud')
plt.title("Transaction Amount vs Balance Difference (Anomaly Detection)")
plt.xlabel("Transaction Amount")
plt.ylabel("Balance Difference (Origin)")
plt.legend()
plt.show()



from sklearn.ensemble import IsolationForest
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Example dataset with transaction features
# df is your dataframe that contains transaction data
# Features: 'amount', 'balanceOrg_diff', 'balanceDest_diff', 'isMerchant', 'type_TRANSFER', 'type_CASH_OUT'

# Train an Isolation Forest model for anomaly detection (fraud detection)
iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)
iso_forest.fit(X_train)

# Predict anomalies on the test set
y_pred_anomalies = iso_forest.predict(X_test)

# In IsolationForest, -1 indicates anomaly (potential fraud), and 1 indicates normal behavior
# We need to ensure `y_pred_anomalies` is aligned with the DataFrame `df`
# If `df` is the full DataFrame, you should use the same indices for test data
# Create a DataFrame for test data to align with indices
df_test = X_test.copy()
df_test['fraud_alert'] = np.where(y_pred_anomalies == -1, 'Alert: Fraud Suspected', 'No Alert')

# Combine with original DataFrame if needed
df_combined = pd.concat([df, df_test[['fraud_alert']]], axis=1, join='inner')

# View potentially fraudulent transactions
fraud_transactions = df_combined[df_combined['fraud_alert'] == 'Alert: Fraud Suspected']

print("Potential Fraudulent Transactions:")
print(fraud_transactions)



plt.figure(figsize=(10, 6))
plt.scatter(df_combined['amount'], df_combined['balanceOrg_diff'], c=(df_combined['fraud_alert'] == 'Alert: Fraud Suspected'), cmap='coolwarm', label='Fraud')
plt.title("Transaction Amount vs Balance Difference (Anomaly Detection)")
plt.xlabel("Transaction Amount")
plt.ylabel("Balance Difference (Origin)")
plt.legend()
plt.show()



df_encoded['isFraud'].value_counts()



# Apply condition to filter rows where 'balanceDest_diff' > 200000
filtered_df = df[df['balanceDest_diff'] > 200000]

# Count the occurrences in the filtered DataFrame
value_counts = filtered_df['balanceDest_diff'].value_counts()

print(value_counts)



# Apply condition to filter rows where 'balanceDest_diff' > 200000
filtered_df = df[df['balanceDest_diff'] > 200000]

# Count the occurrences in the filtered DataFrame
value_counts = filtered_df['balanceDest_diff']

print(value_counts)



# Apply condition to filter rows where 'balanceDest_diff' > 200000
filtered_df = df[df['balanceOrg_diff'] > 200000]

# Count the occurrences in the filtered DataFrame
value_counts = filtered_df['balanceOrg_diff'].value_counts()

print(value_counts)



# Apply multiple conditions using bitwise operators and parentheses
filtered_df = df_encoded[(df_encoded['balanceOrg_diff'] > 200000) & (df_encoded['isFraud'] == 1)]
print(filtered_df)



# Apply multiple conditions using bitwise operators and parentheses
filtered_df = df_encoded[(df_encoded['balanceDest_diff'] > 200000) & (df_encoded['isFraud'] == 1)]

# Optionally, you can count occurrences or view the filtered DataFrame
print(filtered_df)
filtered_df.shape



# Apply multiple conditions using bitwise operators and parentheses
filtered_df = df_encoded[(df_encoded['balanceDest_diff'] > 200000) & (df_encoded['isFraud'] == 1)]

# Optionally, you can count occurrences or view the filtered DataFrame
print(filtered_df)
filtered_df.shape



filtered_df = df_encoded[(df_encoded['type_CASH_OUT'] ==1) & (df_encoded['isFraud'] == 1)]

# Optionally, you can count occurrences or view the filtered DataFrame
print(filtered_df)



from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score

# Assuming you have already trained your model and made predictions
# y_test is the actual test labels
# y_pred is the predicted labels from the model
# y_prob is the predicted probabilities from the model

# Evaluate the basic metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

# Confusion Matrix to analyze false positives and false negatives
conf_matrix = confusion_matrix(y_test, y_pred)

print("Model Evaluation Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Classification report for detailed metrics
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# False Positive Rate (FPR) calculation
tn, fp, fn, tp = conf_matrix.ravel()  # Extract values from confusion matrix
false_positive_rate = fp / (fp + tn)
print(f"False Positive Rate (FPR): {false_positive_rate:.4f}")



v# Simulate previous fraud cases detected
previous_fraud_cases = 200  # example value
current_fraud_cases = (y_test == 1).sum()  # detected by the current system

print(f"Reduction in Fraud Cases: {previous_fraud_cases - current_fraud_cases}")



from sklearn.metrics import roc_curve

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()



import seaborn as sns

# Plot confusion matrix as a heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
plt.title('Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()